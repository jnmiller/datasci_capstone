---
title: "Data Science Capstone 1: Exploration"
author: "J.M."
date: "5/11/2017"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
require(readtext)
require(quanteda)
require(knitr)
setwd("~/coursera/capstone")
```

## Background, Introduction, and Goals

Recall that our goal is to offer mobile phone users predictive suggestions of the next word they will need to type, so they can type more quickly. We'll create the predictor by using real-world examples to "train" a computer program to know what words are most likely to follow after any given sequence of words.

But before we get there, our goal in this report is simply to explore and summarize the main features and patterns in the training text. Analyzing the text beforehand will show us some of the challenges and obstacles that will arise in real-word usage, and help us design a better predictor.

## The Training Dataset

The dataset is the English portion of a collection called the [HC Corpus](https://web-beta.archive.org/web/20160930083655/http://www.corpora.heliohost.org/aboutcorpus.html). The texts were collected from publicly accessible websites in three categories: news, blogs, and Twitter.

It is a relatively large dataset for processing on a laptop, at about 500MB. Let's take a look at other basic stats, like total lines, words, and characters as determined by the built-in system command `wc`. (Here, "words" are non-unique strings of non-whitespace characters.)

```{r, echo=FALSE, cache=TRUE, results="asis"}
wc.out <- system2(command="wc", args=paste0("data/final/en_US/", "*.txt"), stdout=TRUE)
s <- sapply(wc.out, function(x){ strsplit(x, split="[[:space:]]+") } )
wc.df <- data.frame(
    row.names = sapply(s, function(x){x[5]}),
    lines=sapply(s, function(x){x[2]}),
    words=sapply(s, function(x){x[3]}),
    characters=sapply(s, function(x){x[4]})
)
kable(wc.df)
```

The blogs subset alone has nearly 900,000 lines, over 37 million (non-unique) words, and 210 million characters. The entire set has 4.3 million lines, 102 million words, and 583 million characters. By comparison, English translations of _War and Peace_ have fewer than 600,000 words. So this dataset has the amount of text equivalent to over 170 _War and Peace_ sized books. A laptop may take several minutes to analyze this much text, so as we develop our model we may want to experiment with a random 1% sample for faster turnaround times.

## Word Frequencies

If we are going to predict words, we should really define what a "word" means for this project. To a computer, text is just a stream of characters, including punctuation and spaces. It needs rules to separate this stream into logical units. This process is called "tokenization" and the resulting units are called "tokens". Tokens are not just typical "words"; depending on your settings they can include whitespace (spaces, tabs, newlines), punctuation marks, numbers, acronyms, email addresses, proper names, URLs, etc. Even setting options to ignore whitespace and punctuation, this diversity will be challenging for our prediction model, so we should understand the extent of the problem. What size of vocabulary will our model need?

```{r, echo=FALSE, cache=TRUE}
bigcorp <- corpus(readtext(paste0("data/final/en_US/", "*.txt")))
```

The following table shows a better count of unique tokens ("Types") versus the total tokens ("Tokens"), not including whitespace or punctuation. Its token count is slightly different than the rough `wc` count above, due to different tokenization rules.

```{r, echo=FALSE, cache=TRUE, results='asis'}
bigcorp.summary <- summary(bigcorp, showmeta=TRUE, remove_punct=TRUE, verbose=FALSE)
bigcorp.summary$Text <- NULL
kable(bigcorp.summary)
```

The "News" set has the smallest vocabulary of 431k unique tokens ("Types" or "terms"). These include capitalized and uncapitalized words counted separately, proper names, numbers, etc. This is compared to over 34 million total tokens. The Twitter dataset has a much more diverse vocabulary (even with a smaller total token count). I suspect the Twitter dataset has many more abbreviations, slang, misspellings, URLs, usernames, hashtags, etc. that increase its count of unique terms.

Let's take a closer look at the most common terms in the dataset, and we'll remove punctuation and lower-case everything to reduce the size:

```{r echo=FALSE, cache=TRUE}
bigmat <- dfm(bigcorp, tolower=TRUE, remove_punct = TRUE)
```

```{r echo=FALSE}
topfeatures(bigmat, 100)
```

It isn't a surprise that "the" is most common, with 4.8 million uses, and frequency drops off quickly with the 50th ranked term "can" having only 247k uses (19x fewer) and the 100th ranked "us" only 119k (40x fewer).

Let's visualize this exponential dropoff with a plot of term frequency rank vs. frequency for the top 1000 terms (which nicely illustrates "Zipf's Law"):

```{r echo=FALSE}
feat1000 <- topfeatures(bigmat, 1000)
plot(x=seq(length(feat1000)), y=feat1000, type="l", xlab="Term Frequency Rank", ylab="Term Frequency")
```

If we log10-transform both rank and frequency, the relationship between them becomes nicely linear:

```{r echo=FALSE}
plot(x=log10(seq(length(feat1000))), y=log10(feat1000), main="Log-rank vs. log-frequency, top 1000 terms", xlab="log10(rank)", ylab="log10(freq)")
```

Another helpful view is a histogram of log-frequency:

```{r echo=FALSE}
hist(log10(colSums(bigmat)), main="Number of terms by log10(frequency)", xlab="log10(word frequency)", ylab="Number of terms")
```

All of this is good news for our prediction problem. Above, the largest bars between 0 and 1 mean that most unique words ("terms") in our dataset appear less than 10 times, and of those, the vast majority, in the first bar, appear 3 or fewer times! In fact, out of a total vocabulary of 925,000 terms, 812,000 appear fewer than 10 times, and a vocabulary of just 25,000 terms covers more than 95% of the words we'd see in any randomly selected chunk of text. Our predictive accuracy won't suffer much if we just ignore all the rare terms.

To help us understand this better, let's take a look at a sample of the rarest terms (appearing only once):

```{r echo=FALSE}
feat <- colSums(bigmat)
names(feat[feat==1][1:100])
```

There are numbers, foreign terms, hyphenated terms (some of which may be parts of URLs), people/place names ("woodbyrne"), exclamations ("hahaha..."), misspellings ("pajahmas"), and invented words ("resort-y", "yawnology"). While each one of these terms is rare, as a group they are quite common, so we'll need a way to deal with them.

## Bad Words

Out of curiosity, I scanned the text for some vulgar words, curse words, and slurs. Some appeared more than 10,000 times. We will definitely need a blacklist and strategy to ensure our algorithm doesn't offer them as suggestions.

## N-Grams

N-grams are sequences of N consecutive words; 1-grams are single words, 2-grams are sets of 2 consecutive words, etc. Since our initial model will use n-grams, we should get some idea of 2- and 3-gram frequency too.

Since the number of unique n-grams explodes with higher "n" and larger texts, we'll do this part of the analysis on a 1% sample of the dataset.

```{r include=FALSE, cache=TRUE}
samplecorp <- corpus(readtext(paste0("data/final/en_US_sample/", "*.txt")))
```

```{r include=FALSE, cache=TRUE}
unigram.mat <- dfm(samplecorp, ngrams=1, tolower=TRUE, remove_punct=TRUE)
bigram.mat <- dfm(samplecorp, ngrams=2, tolower=TRUE, remove_punct=TRUE)
trigram.mat <- dfm(samplecorp, ngrams=3, tolower=TRUE, remove_punct=TRUE)
```

```{r echo=FALSE, cache=TRUE}
print(paste0("Distinct 1-grams in sample: ", length(colSums(unigram.mat))))
print(paste0("Distinct 2-grams in sample: ", length(colSums(bigram.mat))))
print(paste0("Distinct 3-grams in sample: ", length(colSums(trigram.mat))))
```

Histogram of 2-grams by log10-frequency:

```{r echo=FALSE}
hist(log10(colSums(bigram.mat)), main="2-gram count by log10-frequency", xlab="log10-frequency", ylab="Number of 2-grams")
```

Histogram of 3-grams by log10-frequency:

```{r echo=FALSE}
hist(log10(colSums(trigram.mat)), main="3-gram count by log10-frequency", xlab="log10-frequency", ylab="Number of 3-grams")
```

From the shape of the histograms, it appears the 2-grams have an even higher proportion of very rare terms than single terms, and 3-grams are even worse. It's intuitive that the longer the n-gram (larger "n"), the more likely each n-gram is to be completely unique. 

## Summary of Findings

* **There are a lot of very rare terms in real-world text**. Attempting to predict them all (including numbers, URLS, etc.) is not worthwhile.
* **However, a relatively small vocabulary still covers most text.** In any given chunk of text, most of the words are very common words, which seems very obvious in hindsight. And in fact, a naive predictive "model" that always suggests the three most common terms ("the", "to", "and") is expected to be correct about 10% of the time -- a good baseline to measure model performance.
* **We'll need to carefully define "word"**. How do we choose our vocabulary? Should we treat capitalized words differently? When our input text includes out-of-vocabulary terms including numbers, URLs, emojis, Twitter hashtags, etc. -- what should we do with them?
* **N-grams with larger N tend to be more skewed to low-frequency terms.**  The longer an n-gram, the more rare it is likely to be.
* **We'll need to decide how to handle punctuation, sentence boundaries, and capitalization in our input**. A lot of computer text processing tries to simplify text by stripping out punctuation and lowercasing all letters. However, some of those features could probably be relevant for improving prediction, so we'll need to revisit the issue.
* **Real world language needs a cleanup**: We'll have to avoid predicting vulgar words or slurs.

## Plans for the Prediction Algorithm

### Premodeling and Data Cleaning

The data will need to be tokenized. Initially we'll discard all punctuation and probably lower-case all words to simplify the data as much as possible.

In addition, during development we'll limit the data to a 1% random sample to speed up the feedback loop.

### Validation and Scoring

We'll set aside some data (not in the training set) to test our model. We will take the data as input one n-gram at a time and attempt to predict the next word with 3 suggestions. If any of our 3 suggestions are correct, we consider our prediction correct. The total score will be percent of predictions correct.

### Limiting Vocabulary

The frequency data shows that for reasonable performance we'll need to have a strategy to simplify the dataset's vocabulary. There are several ways to do this, but to start we can just using the most frequent terms. From the rank/frequency data, to have our model know 95+% of non-unique words in the training dataset, we only need to remember the top 25,000 most frequent terms.

We can also try a spelling-correction library, lower-casing all the words, or "stemming" words (e.g., reducing all instances of "run", "runs", "ran", "running" to just "run"); although these techniques might hurt accuracy too.

In addition, we could replace out-of-vocabulary words with a single pseudo-word (like "OOV") for training purposes, to reduce the number of unique terms and n-grams we have to store. We won't offer this placeholder as a suggestion, but still use it as an input.

### Basic N-gram Model

We'll start with a basic n-gram model. Just like with single-term vocabulary, we'll likely need to limit the number of n-grams we use, since so many are rare.

Then we take the last n-1 words of the input, and predict using the three most frequent n-grams whose first n-1 words match. If no n-grams are found, we can fall back to (n-1)-grams, and so on, until our last resort which will be to just predict "the", "to", and "and".
